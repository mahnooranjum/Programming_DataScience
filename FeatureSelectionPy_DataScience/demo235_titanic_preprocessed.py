# -*- coding: utf-8 -*-
"""Demo235_Titanic_Preprocessed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GKrV52aai37FvffL2jNjRfnT3YYS4ENB
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from itertools import product

"""## Get  Dataset"""

def get_classification(i,j,stepsize, X, y, classifiers,texts):
  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
  
  xx, yy = np.meshgrid(np.arange(x_min, x_max, stepsize),
                       np.arange(y_min, y_max, stepsize))

  f, axarr = plt.subplots(i,j, sharex='col', sharey='row', figsize=(10, 8))

  for idx, clf, tt in zip(product([0, i-1], [0, j-1]),
                          classifiers,
                          texts):
      Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
      Z = Z.reshape(xx.shape)

      axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)
      axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y,
                                    s=20, edgecolor='k')
      axarr[idx[0], idx[1]].set_title(tt)

  plt.show()
  return classifiers

def report_classification( X, y,classifiers,texts):
  from sklearn.metrics import accuracy_score
  results = {}
  for clf, tt in zip(     classifiers,
                          texts):
      y_pred = clf.predict(X)
      results[tt] = accuracy_score(y, y_pred)


  return results

from google.colab import drive
drive.mount("/content/gdrive")

data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/FeatureSelection/train_titanic.csv')

data.keys()

y = data.Survived
X = data.drop(columns=['Survived'])

X.head()

y.head()

"""# Analyze"""

X = X.drop(columns=['Name', 'Ticket'])

X.head()

X.isnull().sum()

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
temp = encoder.fit_transform(X['Sex'].values.reshape(-1,1))
X['Sex'] = temp
X.head()

def impute(df, columns, dft):
    df_temp = df.copy()
    for column in columns:
      df_temp[column] = df_temp[column].apply(lambda x: np.random.choice(dft[column].dropna().values) if pd.isnull(x) else x)
    return df_temp

X['Embarked'].unique()

X = impute(X, ['Embarked'], X)
encoder = LabelEncoder()
temp = encoder.fit_transform(X['Embarked'].values.reshape(-1,1))
X['Embarked'] = temp
X.head()

mapper = {k:i for i, k in enumerate(X['Cabin'].unique(), 0)} 
# mapper[np.nan] = 'M'
X['Cabin'] = X['Cabin'].map(mapper)
X.head()

# TRAIN TEST SPLIT
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4)

from sklearn.impute import SimpleImputer
obj = SimpleImputer(missing_values = np.nan, strategy= 'most_frequent')
X_train = obj.fit_transform(X_train)
X_test = obj.transform(X_test)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

y.head()

from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
classifiers = [
    DecisionTreeClassifier(max_depth=4).fit(X_train, y_train),
    KNeighborsClassifier(n_neighbors=7).fit(X_train, y_train),
    SVC(gamma=.1, kernel='rbf', probability=True).fit(X_train, y_train),
    LogisticRegression().fit(X_train, y_train)
    ]
texts = [    "DecisionTreeClassifier",
              "KNeighborsClassifier",
              "SVC",
              "LogisticRegression"]

# classifiers = get_classification(2,2,0.1, X_test, y_test, classifiers, texts)

report_classification( X_test, y_test,classifiers,texts)