# -*- coding: utf-8 -*-
"""Demo220_Filter_Nature_Constant.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uqKFzFvhfQYAkK3pduRtioF7d3KhPv-C

## Constant

*   Show the same value for all the observations of the dataset
*   Provide no information
*   Identifying and remove them
*   Enhance interpretability
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from google.colab import drive
drive.mount("/content/gdrive")

data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/FeatureSelection/train_santander.csv', nrows = 5000)

data.keys()

y = data.TARGET
X = data.drop(columns=['TARGET'])

X.head()

y.head()

"""# Analyze"""

objs = []
nums = []
for i in X.columns:
  if X[i].dtype == 'O':
    objs.append(i)
  else:
    nums.append(i)

print(nums)
print("===")
print(objs)

X.isnull().sum().sum()

[col for col in data.columns if data[col].isnull().sum() > 0]

"""### Important

In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfitting.
"""

# TRAIN TEST SPLIT
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

X_train.shape, X_test.shape

"""## Variance threshold 
By default, it removes all zero-variance features, i.e., features that have the same value in all samples.
"""

obj = VarianceThreshold(threshold=0)
obj.fit(X_train)  # find features with zero variance

# get_support is a boolean vector 
# indicates which features are retained
# summation gives the features that ain't constant 
sum(obj.get_support())

# Slice notation for columns
len(X_train.columns[obj.get_support()])

# Print the constants 
[x for x in X_train.columns if x not in X_train.columns[obj.get_support()]]

len([x for x in X_train.columns if x not in X_train.columns[obj.get_support()]])

# Proof of constants 
X_train['saldo_medio_var29_hace2'].unique()

X_train = obj.transform(X_train)
X_test = obj.transform(X_test)

X_train.shape, X_test.shape

"""### Try it yourself 
Code from scratch
"""

data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/FeatureSelection/train_santander.csv', nrows = 5000)

y = data.TARGET
X = data.drop(columns=['TARGET'])
X.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

X_train.shape, X_test.shape

constant_features = [
    col for col in X_train.columns if X_train[col].std() == 0
]
len(constant_features)

# Drop them columns 
X_train.drop(labels=constant_features, axis=1, inplace=True)
X_test.drop(labels=constant_features, axis=1, inplace=True)

X_train.shape, X_test.shape

"""

## Based on the number of values """

data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/FeatureSelection/train_santander.csv', nrows = 5000)
y = data.TARGET
X = data.drop(columns=['TARGET'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

X_train.shape, X_test.shape

# Find columns with only one value
constant_features = [
    col for col in X_train.columns if len(X_train[col].unique()) == 1
]
len(constant_features)

